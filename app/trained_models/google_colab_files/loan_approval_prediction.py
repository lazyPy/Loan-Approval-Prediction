# -*- coding: utf-8 -*-
"""loan_approval_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GU994DL8m8SWMcEILsumuYdH6619uxwr
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from scipy.stats import randint
import warnings
warnings.filterwarnings('ignore')

def load_and_preprocess_data(file_path):
    """Load and preprocess the loan approval dataset."""
    # Load the dataset
    df = pd.read_csv(file_path)
    
    # Convert 'Loan Status' to binary (1 for Approved, 0 for Declined)
    df['Loan Status'] = df['Loan Status'].apply(lambda x: 1 if x == 'Approved' else 0)
    
    # Identify categorical and numerical columns
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
    numerical_cols.remove('Loan Status')  # Remove target variable
    
    # Calculate class weights for imbalance
    approved_count = df['Loan Status'].sum()
    declined_count = len(df) - approved_count
    total_samples = len(df)
    n_classes = 2
    class_weights = {
        0: total_samples / (n_classes * declined_count),
        1: total_samples / (n_classes * approved_count)
    }
    
    # Create feature matrix X and target vector y
    X = df.drop('Loan Status', axis=1)
    y = df['Loan Status']
    
    return X, y, numerical_cols, categorical_cols, class_weights

def create_pipeline(numerical_cols, categorical_cols, class_weights):
    """Create a preprocessing and model pipeline."""
    # Define preprocessing for numerical and categorical features
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='median')),
                ('scaler', StandardScaler())
            ]), numerical_cols),
            ('cat', Pipeline(steps=[
                ('imputer', SimpleImputer(strategy='most_frequent')),
                ('onehot', OneHotEncoder(handle_unknown='ignore'))
            ]), categorical_cols)
        ])
    
    # Create pipeline with preprocessing and model
    dt_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', DecisionTreeClassifier(random_state=42, class_weight=class_weights))
    ])
    
    return dt_pipeline

def train_model(pipeline, X_train, y_train):
    """Train the model using RandomizedSearchCV."""
    # Define hyperparameters for randomized search
    param_distributions = {
        'classifier__max_depth': [3, 5, 7, 10, None],
        'classifier__min_samples_split': randint(2, 20),
        'classifier__min_samples_leaf': randint(1, 10),
        'classifier__criterion': ['gini', 'entropy']
    }
    
    # Perform randomized search with cross-validation
    random_search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_distributions,
        n_iter=20,
        cv=5,
        scoring='f1',
        n_jobs=-1,
        random_state=42
    )
    
    # Train the model
    random_search.fit(X_train, y_train)
    return random_search.best_estimator_

def evaluate_model(model, X_test, y_test):
    """Evaluate the model and print metrics."""
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # Calculate and print metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    print("\nModel Evaluation Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc:.4f}")
    
    return y_pred, y_pred_proba

def plot_confusion_matrix(y_test, y_pred, output_path='confusion_matrix.png'):
    """Plot and save the confusion matrix."""
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Declined', 'Approved'],
                yticklabels=['Declined', 'Approved'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

def plot_feature_importance(model, numerical_cols, categorical_cols, output_path='feature_importance.png'):
    """Plot and save feature importance."""
    # Extract feature names and importance
    dt_classifier = model.named_steps['classifier']
    preprocessor = model.named_steps['preprocessor']
    cat_features = preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(categorical_cols)
    feature_names = numerical_cols + list(cat_features)
    
    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': dt_classifier.feature_importances_
    }).sort_values('Importance', ascending=False).reset_index(drop=True)
    
    # Plot top 5 important features
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=importance_df.head(5))
    plt.title('Top 5 Feature Importance')
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return importance_df

def save_model(model, filepath='loan_approval_model.joblib'):
    """Save the trained model to a file."""
    joblib.dump(model, filepath)
    print(f"\nTrained model saved to '{filepath}'")

def main():
    """Main function to orchestrate the loan approval prediction workflow."""
    # Set plot style
    plt.style.use('ggplot')
    
    # Load and preprocess data
    X, y, numerical_cols, categorical_cols, class_weights = load_and_preprocess_data("LoanApprovalData.csv")
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Create pipeline
    pipeline = create_pipeline(numerical_cols, categorical_cols, class_weights)
    
    # Train model
    model = train_model(pipeline, X_train, y_train)
    
    # Evaluate model
    y_pred, y_pred_proba = evaluate_model(model, X_test, y_test)
    
    # Plot results
    plot_confusion_matrix(y_test, y_pred)
    importance_df = plot_feature_importance(model, numerical_cols, categorical_cols)
    
    # Save model
    save_model(model)
    
    return model, importance_df

if __name__ == "__main__":
    main()